基于下面的信息，给出英文技术博客文章（面向欧美用户，基于 Google Adsense赚钱）：
文章为主，代码为辅。
要有图表和表格。

Reference Title: Spring AI Observability: Monitoring, Logging & Tracing
Reference Keywords: spring ai monitoring
Target Word Count: 6000-7000

markdown 摘要信息的格式如下：
---
title: "xxxx"
date: "2025-xx-xx"
author: "SpringDevPro Team"
tags: [xxx, xxx]
categories: [Spring AI]
description: "xxxx"
keywords: "xxx, xxx"
featured_image: "xxxx"
reading_time: "xx min read"
difficulty: "xx"
---

---
title: "Spring AI Observability: Complete Guide to Monitoring, Logging & Tracing"
date: "2025-11-20"
author: "SpringDevPro Team"
tags: [spring-ai, observability, monitoring, logging, tracing, metrics]
categories: [Spring AI, DevOps]
description: "Master observability in Spring AI applications with comprehensive monitoring, distributed tracing, intelligent logging, and real-time metrics. Learn to debug AI systems, track performance, and ensure production reliability."
keywords: "spring ai monitoring, ai observability, llm tracing, ai metrics, distributed tracing, spring boot monitoring, ai logging"
featured_image: "images/spring-ai-observability.png"
reading_time: "34 min read"
difficulty: "Intermediate"
---

# Spring AI Observability: Complete Guide to Monitoring, Logging & Tracing

## The Day Everything Went Silent

It was 2:47 AM when the alerts started flooding in. A major e-commerce platform's AI-powered recommendation engine had stopped responding. Customer support chatbots were timing out. Product search was returning empty results.

The on-call engineer scrambled to diagnose the issue. But there was a problem: **they had no visibility into what was happening inside their AI system**.

- No logs showing which prompts were failing
- No metrics tracking token usage or latency
- No traces connecting user requests to OpenAI API calls
- No alerts warning them of degrading performance

They were flying blind. It took **6 hours** to discover that OpenAI had quietly changed their rate limits, and the application was silently failing without proper error handling or monitoring.

**Total cost:** $120,000 in lost revenue, plus immeasurable damage to customer trust.

This scenario plays out more often than you'd think. AI applications introduce new complexities that traditional monitoring tools weren't designed to handle:

- **Non-deterministic behavior**: The same input can produce different outputs
- **External dependencies**: You rely on third-party AI APIs with their own failure modes
- **Complex data flows**: Prompts, embeddings, vector searches, and LLM calls create intricate execution paths
- **Cost implications**: Every API call costs money; you need to track usage
- **Quality metrics**: How do you measure if an AI response is "good"?

**This guide will show you how to build comprehensive observability into your Spring AI applications** so you'll never be caught in the dark again.

## The Three Pillars of Observability

Traditional monitoring focuses on known failure modes. **Observability** goes further—it helps you understand **unknown problems** in complex systems.

### Metrics: What's Happening?

Quantitative measurements that answer: "What is the system doing right now?"

**Examples:**
- Requests per second
- Average response time
- Token usage
- Cache hit rate
- Error rate

### Logs: Why Did It Happen?

Contextual information about specific events that answer: "What happened in this particular case?"

**Examples:**
- User query: "How do I reset my password?"
- AI response: "To reset your password, visit..."
- Model used: gpt-4o-mini
- Latency: 847ms
- Tokens used: 234

### Traces: How Did It Flow?

End-to-end request tracking that answers: "What was the complete path of this request?"

**Examples:**
```
User Request → Input Validation → Cache Check (miss) → 
Vector Search → Prompt Building → OpenAI API Call → 
Response Filtering → Cache Store → Return to User
```

**All three together** give you complete observability.

## Observability Architecture for Spring AI

```
┌─────────────────────────────────────────────────────────────────┐
│                     Spring AI Application                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                   │
│  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐  │
│  │   Chat       │      │   Embedding  │      │   Vector     │  │
│  │   Service    │──────│   Service    │──────│   Store      │  │
│  └──────────────┘      └──────────────┘      └──────────────┘  │
│         │                      │                      │          │
│         └──────────────────────┴──────────────────────┘          │
│                                │                                  │
│                    ┌───────────▼──────────┐                      │
│                    │  Observability Layer  │                      │
│                    └───────────┬──────────┘                      │
└────────────────────────────────┼───────────────────────────────┘
                                 │
          ┌──────────────────────┼──────────────────────┐
          │                      │                       │
          ▼                      ▼                       ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │ Micrometer│          │   SLF4J   │          │  OpenTel │
    │ Metrics   │          │   Logs    │          │ Tracing  │
    └─────┬────┘          └─────┬────┘          └─────┬────┘
          │                      │                       │
          ▼                      ▼                       ▼
    ┌──────────┐          ┌──────────┐          ┌──────────┐
    │Prometheus │          │  Loki/    │          │  Jaeger/ │
    │          │          │  ELK      │          │  Zipkin  │
    └─────┬────┘          └─────┬────┘          └─────┬────┘
          │                      │                       │
          └──────────────────────┴───────────────────────┘
                                 │
                                 ▼
                          ┌──────────┐
                          │  Grafana  │
                          │ Dashboard │
                          └──────────┘
```

## Setting Up the Foundation

### Dependencies

```xml
<!-- pom.xml -->
<dependencies>
    <!-- Spring AI -->
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-openai-spring-boot-starter</artifactId>
        <version>1.0.0-M3</version>
    </dependency>
    
    <!-- Observability -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-actuator</artifactId>
    </dependency>
    
    <!-- Metrics -->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-registry-prometheus</artifactId>
    </dependency>
    
    <!-- Distributed Tracing -->
    <dependency>
        <groupId>io.micrometer</groupId>
        <artifactId>micrometer-tracing-bridge-otel</artifactId>
    </dependency>
    
    <dependency>
        <groupId>io.opentelemetry</groupId>
        <artifactId>opentelemetry-exporter-zipkin</artifactId>
    </dependency>
    
    <!-- Structured Logging -->
    <dependency>
        <groupId>net.logstash.logback</groupId>
        <artifactId>logstash-logback-encoder</artifactId>
        <version>7.4</version>
    </dependency>
</dependencies>
```

### Configuration

```yaml
# application.yml
spring:
  application:
    name: spring-ai-chatbot
    
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      chat:
        options:
          model: gpt-4o-mini
          temperature: 0.7

# Actuator endpoints
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus,traces
  endpoint:
    health:
      show-details: always
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: ${spring.application.name}
      environment: ${ENVIRONMENT:dev}
  tracing:
    sampling:
      probability: 1.0  # Sample 100% in dev, reduce in prod
  observations:
    annotations:
      enabled: true

# Logging
logging:
  level:
    root: INFO
    org.springframework.ai: DEBUG
    com.yourcompany.ai: DEBUG
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
```

## Pillar 1: Comprehensive Metrics

### Custom Metrics Service

```java
@Service
@Slf4j
public class AIMetricsService {
    
    private final MeterRegistry meterRegistry;
    
    // Counters
    private final Counter aiRequestsTotal;
    private final Counter aiRequestsSuccess;
    private final Counter aiRequestsFailure;
    private final Counter tokensUsedTotal;
    private final Counter cacheHits;
    private final Counter cacheMisses;
    
    // Timers
    private final Timer aiRequestDuration;
    private final Timer promptBuildDuration;
    private final Timer llmCallDuration;
    private final Timer embeddingDuration;
    
    // Gauges
    private final AtomicInteger activeRequests;
    private final AtomicDouble currentCostRate;
    
    // Distributions
    private final DistributionSummary promptTokens;
    private final DistributionSummary responseTokens;
    
    public AIMetricsService(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        
        // Initialize counters
        this.aiRequestsTotal = Counter.builder("ai.requests.total")
            .description("Total number of AI requests")
            .register(meterRegistry);
        
        this.aiRequestsSuccess = Counter.builder("ai.requests.success")
            .description("Successful AI requests")
            .register(meterRegistry);
        
        this.aiRequestsFailure = Counter.builder("ai.requests.failure")
            .description("Failed AI requests")
            .tag("type", "unknown")
            .register(meterRegistry);
        
        this.tokensUsedTotal = Counter.builder("ai.tokens.used.total")
            .description("Total tokens consumed")
            .register(meterRegistry);
        
        this.cacheHits = Counter.builder("ai.cache.hits")
            .description("Cache hits")
            .register(meterRegistry);
        
        this.cacheMisses = Counter.builder("ai.cache.misses")
            .description("Cache misses")
            .register(meterRegistry);
        
        // Initialize timers
        this.aiRequestDuration = Timer.builder("ai.request.duration")
            .description("AI request duration")
            .publishPercentiles(0.5, 0.95, 0.99)
            .register(meterRegistry);
        
        this.promptBuildDuration = Timer.builder("ai.prompt.build.duration")
            .description("Time to build prompts")
            .register(meterRegistry);
        
        this.llmCallDuration = Timer.builder("ai.llm.call.duration")
            .description("LLM API call duration")
            .publishPercentiles(0.5, 0.95, 0.99)
            .register(meterRegistry);
        
        this.embeddingDuration = Timer.builder("ai.embedding.duration")
            .description("Embedding generation duration")
            .register(meterRegistry);
        
        // Initialize gauges
        this.activeRequests = new AtomicInteger(0);
        Gauge.builder("ai.requests.active", activeRequests, AtomicInteger::get)
            .description("Currently active AI requests")
            .register(meterRegistry);
        
        this.currentCostRate = new AtomicDouble(0.0);
        Gauge.builder("ai.cost.rate.current", currentCostRate, AtomicDouble::get)
            .description("Current cost rate ($/hour)")
            .register(meterRegistry);
        
        // Initialize distributions
        this.promptTokens = DistributionSummary.builder("ai.tokens.prompt")
            .description("Prompt token distribution")
            .publishPercentiles(0.5, 0.95, 0.99)
            .register(meterRegistry);
        
        this.responseTokens = DistributionSummary.builder("ai.tokens.response")
            .description("Response token distribution")
            .publishPercentiles(0.5, 0.95, 0.99)
            .register(meterRegistry);
    }
    
    /**
     * Record complete AI request metrics
     */
    public void recordAIRequest(AIRequestMetrics metrics) {
        // Increment total requests
        aiRequestsTotal.increment();
        
        // Record success/failure
        if (metrics.isSuccess()) {
            aiRequestsSuccess.increment();
        } else {
            Counter.builder("ai.requests.failure")
                .tag("type", metrics.getErrorType())
                .register(meterRegistry)
                .increment();
        }
        
        // Record duration
        aiRequestDuration.record(metrics.getDuration());
        
        // Record tokens
        if (metrics.getPromptTokens() > 0) {
            promptTokens.record(metrics.getPromptTokens());
            tokensUsedTotal.increment(metrics.getPromptTokens());
        }
        
        if (metrics.getResponseTokens() > 0) {
            responseTokens.record(metrics.getResponseTokens());
            tokensUsedTotal.increment(metrics.getResponseTokens());
        }
        
        // Record model usage
        Counter.builder("ai.model.usage")
            .tag("model", metrics.getModel())
            .register(meterRegistry)
            .increment();
        
        // Record cost
        if (metrics.getCost() > 0) {
            Counter.builder("ai.cost.total")
                .description("Total AI cost in USD")
                .tag("model", metrics.getModel())
                .register(meterRegistry)
                .increment(metrics.getCost());
            
            updateCostRate(metrics.getCost());
        }
        
        // Record cache status
        if (metrics.isCacheHit()) {
            cacheHits.increment();
        } else {
            cacheMisses.increment();
        }
    }
    
    /**
     * Track active requests
     */
    public void incrementActiveRequests() {
        activeRequests.incrementAndGet();
    }
    
    public void decrementActiveRequests() {
        activeRequests.decrementAndGet();
    }
    
    /**
     * Record LLM-specific metrics
     */
    public void recordLLMCall(String model, Duration duration, 
                             int inputTokens, int outputTokens) {
        
        llmCallDuration.record(duration);
        
        Counter.builder("ai.llm.calls")
            .tag("model", model)
            .register(meterRegistry)
            .increment();
        
        promptTokens.record(inputTokens);
        responseTokens.record(outputTokens);
        
        // Calculate and record cost
        double cost = calculateCost(model, inputTokens, outputTokens);
        Counter.builder("ai.cost.total")
            .tag("model", model)
            .register(meterRegistry)
            .increment(cost);
    }
    
    /**
     * Record embedding generation
     */
    public void recordEmbedding(Duration duration, int tokens) {
        embeddingDuration.record(duration);
        
        Counter.builder("ai.embeddings.generated")
            .register(meterRegistry)
            .increment();
        
        DistributionSummary.builder("ai.embedding.tokens")
            .register(meterRegistry)
            .record(tokens);
    }
    
    /**
     * Record vector search metrics
     */
    public void recordVectorSearch(Duration duration, int results) {
        Timer.builder("ai.vector.search.duration")
            .register(meterRegistry)
            .record(duration);
        
        DistributionSummary.builder("ai.vector.search.results")
            .register(meterRegistry)
            .record(results);
    }
    
    private double calculateCost(String model, int inputTokens, int outputTokens) {
        // Model pricing (per 1K tokens)
        Map<String, double[]> pricing = Map.of(
            "gpt-4-turbo", new double[]{0.01, 0.03},
            "gpt-4o", new double[]{0.0025, 0.01},
            "gpt-4o-mini", new double[]{0.00015, 0.0006},
            "gpt-3.5-turbo", new double[]{0.0005, 0.0015}
        );
        
        double[] prices = pricing.getOrDefault(model, new double[]{0.01, 0.03});
        
        return (inputTokens / 1000.0 * prices[0]) + 
               (outputTokens / 1000.0 * prices[1]);
    }
    
    private void updateCostRate(double cost) {
        // Update rolling cost rate (simplified)
        double currentRate = currentCostRate.get();
        currentCostRate.set(currentRate * 0.9 + cost * 0.1);
    }
    
    @Data
    @Builder
    public static class AIRequestMetrics {
        private boolean success;
        private Duration duration;
        private int promptTokens;
        private int responseTokens;
        private String model;
        private double cost;
        private boolean cacheHit;
        private String errorType;
    }
}
```

### Metrics Integration

```java
@Service
@Slf4j
public class ObservableChatService {
    
    private final ChatClient chatClient;
    private final AIMetricsService metrics;
    private final CacheService cache;
    
    /**
     * Chat endpoint with full observability
     */
    public String chat(String userMessage, String userId) {
        metrics.incrementActiveRequests();
        
        Instant start = Instant.now();
        boolean success = false;
        boolean cacheHit = false;
        int promptTokens = 0;
        int responseTokens = 0;
        String model = "gpt-4o-mini";
        String errorType = null;
        
        try {
            // Check cache
            Optional<String> cached = cache.get(userMessage);
            if (cached.isPresent()) {
                cacheHit = true;
                success = true;
                return cached.get();
            }
            
            // Build prompt
            String prompt = buildPrompt(userMessage);
            promptTokens = estimateTokens(prompt);
            
            // Call LLM
            Instant llmStart = Instant.now();
            ChatResponse response = chatClient.prompt()
                .user(prompt)
                .options(OpenAiChatOptions.builder()
                    .withModel(model)
                    .build())
                .call()
                .chatResponse();
            
            Duration llmDuration = Duration.between(llmStart, Instant.now());
            
            String responseText = response.getResult().getOutput().getContent();
            responseTokens = estimateTokens(responseText);
            
            // Record LLM-specific metrics
            metrics.recordLLMCall(
                model,
                llmDuration,
                promptTokens,
                responseTokens
            );
            
            // Cache response
            cache.put(userMessage, responseText);
            
            success = true;
            return responseText;
            
        } catch (Exception e) {
            errorType = e.getClass().getSimpleName();
            log.error("Chat request failed", e);
            throw e;
            
        } finally {
            Duration totalDuration = Duration.between(start, Instant.now());
            
            // Record comprehensive metrics
            metrics.recordAIRequest(
                AIMetricsService.AIRequestMetrics.builder()
                    .success(success)
                    .duration(totalDuration)
                    .promptTokens(promptTokens)
                    .responseTokens(responseTokens)
                    .model(model)
                    .cost(calculateCost(model, promptTokens, responseTokens))
                    .cacheHit(cacheHit)
                    .errorType(errorType)
                    .build()
            );
            
            metrics.decrementActiveRequests();
        }
    }
}
```

### Key Metrics to Track

| Metric Category | Specific Metrics | Why It Matters |
|----------------|------------------|----------------|
| **Request Volume** | Requests/sec, Daily total | Capacity planning, scaling |
| **Latency** | p50, p95, p99 response time | User experience, SLA compliance |
| **Success Rate** | Success %, Error rate | Service reliability |
| **Token Usage** | Tokens/request, Total tokens | Cost optimization |
| **Cache Performance** | Hit rate, Miss rate | Cost savings effectiveness |
| **Model Distribution** | Requests by model | Cost allocation |
| **Cost** | $/hour, $/day, Cost/request | Budget management |
| **Active Requests** | Current concurrent requests | Load monitoring |

## Pillar 2: Intelligent Logging

### Structured Logging Configuration

```xml
<!-- logback-spring.xml -->
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml"/>
    
    <springProperty scope="context" name="appName" source="spring.application.name"/>
    <springProperty scope="context" name="environment" source="ENVIRONMENT" defaultValue="dev"/>
    
    <!-- Console Appender (Development) -->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app":"${appName}","env":"${environment}"}</customFields>
            <includeMdcKeyName>trace_id</includeMdcKeyName>
            <includeMdcKeyName>span_id</includeMdcKeyName>
            <includeMdcKeyName>user_id</includeMdcKeyName>
            <includeMdcKeyName>request_id</includeMdcKeyName>
        </encoder>
    </appender>
    
    <!-- File Appender (Production) -->
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/${appName}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/${appName}-%d{yyyy-MM-dd}.%i.log.gz</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy 
                class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app":"${appName}","env":"${environment}"}</customFields>
        </encoder>
    </appender>
    
    <!-- AI-specific log file -->
    <appender name="AI_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/ai-interactions.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/ai-interactions-%d{yyyy-MM-dd}.log.gz</fileNamePattern>
            <maxHistory>90</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"/>
    </appender>
    
    <logger name="com.yourcompany.ai" level="DEBUG" additivity="false">
        <appender-ref ref="AI_FILE"/>
        <appender-ref ref="CONSOLE"/>
    </logger>
    
    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>
```

### Structured Logging Service

```java
@Service
@Slf4j
public class AILoggingService {
    
    /**
     * Log AI interaction with structured data
     */
    public void logAIInteraction(AIInteractionLog interaction) {
        // Use SLF4J with MDC for structured logging
        MDC.put("user_id", interaction.getUserId());
        MDC.put("request_id", interaction.getRequestId());
        MDC.put("model", interaction.getModel());
        MDC.put("prompt_tokens", String.valueOf(interaction.getPromptTokens()));
        MDC.put("response_tokens", String.valueOf(interaction.getResponseTokens()));
        MDC.put("duration_ms", String.valueOf(interaction.getDuration().toMillis()));
        MDC.put("cost", String.format("%.4f", interaction.getCost()));
        MDC.put("cache_hit", String.valueOf(interaction.isCacheHit()));
        
        try {
            if (interaction.isSuccess()) {
                log.info("AI interaction completed successfully: " +
                    "user={}, model={}, duration={}ms, tokens={}/{}, cost=${}, cache={}",
                    interaction.getUserId(),
                    interaction.getModel(),
                    interaction.getDuration().toMillis(),
                    interaction.getPromptTokens(),
                    interaction.getResponseTokens(),
                    String.format("%.4f", interaction.getCost()),
                    interaction.isCacheHit() ? "HIT" : "MISS"
                );
                
                // Log full interaction details at debug level
                log.debug("AI interaction details: prompt='{}', response='{}'",
                    sanitizeForLogging(interaction.getPrompt()),
                    sanitizeForLogging(interaction.getResponse())
                );
                
            } else {
                log.error("AI interaction failed: " +
                    "user={}, model={}, duration={}ms, error={}",
                    interaction.getUserId(),
                    interaction.getModel(),
                    interaction.getDuration().toMillis(),
                    interaction.getErrorMessage()
                );
            }
            
        } finally {
            MDC.clear();
        }
    }
    
    /**
     * Log prompt injection attempt
     */
    public void logSecurityEvent(SecurityEvent event) {
        MDC.put("user_id", event.getUserId());
        MDC.put("event_type", event.getType().name());
        MDC.put("severity", event.getSeverity().name());
        
        try {
            log.warn("Security event detected: type={}, user={}, severity={}, details={}",
                event.getType(),
                event.getUserId(),
                event.getSeverity(),
                event.getDetails()
            );
            
        } finally {
            MDC.clear();
        }
    }
    
    /**
     * Log cost anomaly
     */
    public void logCostAnomaly(CostAnomaly anomaly) {
        MDC.put("user_id", anomaly.getUserId());
        MDC.put("current_cost", String.format("%.2f", anomaly.getCurrentCost()));
        MDC.put("expected_cost", String.format("%.2f", anomaly.getExpectedCost()));
        MDC.put("deviation", String.format("%.1f%%", anomaly.getDeviationPercent()));
        
        try {
            log.warn("Cost anomaly detected: user={}, current=${}, expected=${}, deviation={}%",
                anomaly.getUserId(),
                anomaly.getCurrentCost(),
                anomaly.getExpectedCost(),
                anomaly.getDeviationPercent()
            );
            
        } finally {
            MDC.clear();
        }
    }
    
    /**
     * Log quality issues
     */
    public void logQualityIssue(QualityIssue issue) {
        MDC.put("request_id", issue.getRequestId());
        MDC.put("issue_type", issue.getType().name());
        MDC.put("model", issue.getModel());
        
        try {
            log.warn("Response quality issue: type={}, model={}, details={}",
                issue.getType(),
                issue.getModel(),
                issue.getDetails()
            );
            
        } finally {
            MDC.clear();
        }
    }
    
    /**
     * Sanitize sensitive data before logging
     */
    private String sanitizeForLogging(String text) {
        if (text == null) {
            return null;
        }
        
        // Remove PII
        String sanitized = text
            // Email addresses
            .replaceAll("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}", "[EMAIL]")
            // Phone numbers
            .replaceAll("\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b", "[PHONE]")
            // Credit cards
            .replaceAll("\\b\\d{4}[- ]?\\d{4}[- ]?\\d{4}[- ]?\\d{4}\\b", "[CARD]")
            // SSN
            .replaceAll("\\b\\d{3}-\\d{2}-\\d{4}\\b", "[SSN]");
        
        // Truncate if too long
        if (sanitized.length() > 500) {
            return sanitized.substring(0, 500) + "... [truncated]";
        }
        
        return sanitized;
    }
    
    @Data
    @Builder
    public static class AIInteractionLog {
        private String userId;
        private String requestId;
        private String model;
        private String prompt;
        private String response;
        private int promptTokens;
        private int responseTokens;
        private Duration duration;
        private double cost;
        private boolean cacheHit;
        private boolean success;
        private String errorMessage;
    }
    
    @Data
    @Builder
    public static class SecurityEvent {
        private String userId;
        private SecurityEventType type;
        private Severity severity;
        private String details;
    }
    
    public enum SecurityEventType {
        PROMPT_INJECTION,
        DATA_EXFILTRATION,
        RATE_LIMIT_EXCEEDED,
        SUSPICIOUS_PATTERN
    }
    
    public enum Severity {
        LOW, MEDIUM, HIGH, CRITICAL
    }
}
```

### Log Aggregation Queries

**Example Loki/LogQL queries:**

```
# Find all failed AI requests in last hour
{app="spring-ai-chatbot"} |= "AI interaction failed" | json | __error__="" | line_format "{{.user_id}}: {{.error}}"

# Calculate average response time by model
avg_over_time({app="spring-ai-chatbot"} |= "AI interaction completed" | json | unwrap duration_ms [5m]) by (model)

# Find expensive requests (>$0.10)
{app="spring-ai-chatbot"} | json | cost > 0.10 | line_format "Expensive request: user={{.user_id}}, cost=${{.cost}}"

# Detect security events
{app="spring-ai-chatbot"} |= "Security event detected" | json | severity="HIGH" or severity="CRITICAL"

# Cache performance
sum(count_over_time({app="spring-ai-chatbot"} | json | cache_hit="true" [1h])) / 
sum(count_over_time({app="spring-ai-chatbot"} | json [1h]))
```

## Pillar 3: Distributed Tracing

### Trace Configuration

```java
@Configuration
public class TracingConfiguration {
    
    /**
     * Custom trace context propagation
     */
    @Bean
    public ObservationHandler<Context> customTracingHandler() {
        return new ObservationHandler<Context>() {
            
            @Override
            public void onStart(Context context) {
                // Add custom tags to trace
                context.put("ai.operation", determineOperation(context));
                context.put("user.id", getCurrentUserId());
            }
            
            @Override
            public void onStop(Context context) {
                // Record final metrics
                recordTraceMetrics(context);
            }
            
            @Override
            public boolean supportsContext(Context context) {
                return true;
            }
        };
    }
}
```

### Instrumented Chat Service

```java
@Service
@Slf4j
public class TracedChatService {
    
    private final ChatClient chatClient;
    private final VectorStore vectorStore;
    private final CacheService cache;
    private final ObservationRegistry observationRegistry;
    
    /**
     * Chat with full distributed tracing
     */
    @Observed(
        name = "ai.chat.request",
        contextualName = "chat-request",
        lowCardinalityKeyValues = {"operation", "chat"}
    )
    public String chat(String userMessage, String userId) {
        
        return Observation.createNotStarted("ai.chat.request", observationRegistry)
            .contextualName("process-chat")
            .lowCardinalityKeyValue("user.id", userId)
            .highCardinalityKeyValue("message.hash", hashMessage(userMessage))
            .observe(() -> {
                
                // Step 1: Check cache (traced)
                Optional<String> cached = checkCache(userMessage);
                if (cached.isPresent()) {
                    return cached.get();
                }
                
                // Step 2: Search for relevant context (traced)
                List<Document> context = searchContext(userMessage);
                
                // Step 3: Build prompt (traced)
                String prompt = buildPrompt(userMessage, context);
                
                // Step 4: Call LLM (traced)
                String response = callLLM(prompt, userId);
                
                // Step 5: Store in cache (traced)
                storeInCache(userMessage, response);
                
                return response;
            });
    }
    
    @Observed(
        name = "ai.cache.check",
        contextualName = "cache-check"
    )
    private Optional<String> checkCache(String message) {
        return Observation.createNotStarted("ai.cache.check", observationRegistry)
            .observe(() -> {
                Optional<String> result = cache.get(message);
                
                // Add trace metadata
                Observation.Scope scope = Observation.getCurrentObservation()
                    .openScope();
                try {
                    scope.getCurrentObservation()
                        .lowCardinalityKeyValue("cache.hit", String.valueOf(result.isPresent()));
                } finally {
                    scope.close();
                }
                
                return result;
            });
    }
    
    @Observed(
        name = "ai.context.search",
        contextualName = "vector-search"
    )
    private List<Document> searchContext(String query) {
        return Observation.createNotStarted("ai.context.search", observationRegistry)
            .observe(() -> {
                Instant start = Instant.now();
                
                List<Document> results = vectorStore.similaritySearch(
                    SearchRequest.query(query)
                        .withTopK(5)
                        .withSimilarityThreshold(0.7)
                );
                
                Duration duration = Duration.between(start, Instant.now());
                
                // Add trace metadata
                Observation.Scope scope = Observation.getCurrentObservation()
                    .openScope();
                try {
                    scope.getCurrentObservation()
                        .highCardinalityKeyValue("results.count", String.valueOf(results.size()))
                        .lowCardinalityKeyValue("duration.ms", String.valueOf(duration.toMillis()));
                } finally {
                    scope.close();
                }
                
                return results;
            });
    }
    
    @Observed(
        name = "ai.prompt.build",
        contextualName = "build-prompt"
    )
    private String buildPrompt(String userMessage, List<Document> context) {
        return Observation.createNotStarted("ai.prompt.build", observationRegistry)
            .observe(() -> {
                StringBuilder prompt = new StringBuilder();
                
                if (!context.isEmpty()) {
                    prompt.append("Context:\n");
                    context.forEach(doc -> 
                        prompt.append(doc.getContent()).append("\n\n")
                    );
                }
                
                prompt.append("User: ").append(userMessage);
                
                String result = prompt.toString();
                
                // Add trace metadata
                int tokens = estimateTokens(result);
                Observation.Scope scope = Observation.getCurrentObservation()
                    .openScope();
                try {
                    scope.getCurrentObservation()
                        .highCardinalityKeyValue("prompt.tokens", String.valueOf(tokens))
                        .lowCardinalityKeyValue("context.docs", String.valueOf(context.size()));
                } finally {
                    scope.close();
                }
                
                return result;
            });
    }
    
    @Observed(
        name = "ai.llm.call",
        contextualName = "openai-api-call"
    )
    private String callLLM(String prompt, String userId) {
        return Observation.createNotStarted("ai.llm.call", observationRegistry)
            .lowCardinalityKeyValue("model", "gpt-4o-mini")
            .lowCardinalityKeyValue("user.id", userId)
            .observe(() -> {
                Instant start = Instant.now();
                
                String response = chatClient.prompt()
                    .user(prompt)
                    .call()
                    .content();
                
                Duration duration = Duration.between(start, Instant.now());
                int responseTokens = estimateTokens(response);
                
                // Add trace metadata
                Observation.Scope scope = Observation.getCurrentObservation()
                    .openScope();
                try {
                    scope.getCurrentObservation()
                        .highCardinalityKeyValue("response.tokens", String.valueOf(responseTokens))
                        .lowCardinalityKeyValue("duration.ms", String.valueOf(duration.toMillis()));
                } finally {
                    scope.close();
                }
                
                return response;
            });
    }
    
    @Observed(
        name = "ai.cache.store",
        contextualName = "cache-store"
    )
    private void storeInCache(String message, String response) {
        Observation.createNotStarted("ai.cache.store", observationRegistry)
            .observe(() -> {
                cache.put(message, response);
                return null;
            });
    }
}
```

### Trace Visualization

**Example trace timeline:**

```
Chat Request (total: 1,247ms)
├─ Cache Check (12ms) [MISS]
├─ Vector Search (234ms)
│  ├─ Generate Embedding (45ms)
│  ├─ Query Vector DB (178ms)
│  └─ Rank Results (11ms)
├─ Build Prompt (8ms)
├─ OpenAI API Call (967ms)
│  ├─ HTTP Request (15ms)
│  ├─ LLM Processing (928ms)
│  └─ HTTP Response (24ms)
└─ Cache Store (26ms)
```

### Critical Spans to Track

| Span Name | What It Measures | Alert Threshold |
|-----------|------------------|-----------------|
| **ai.chat.request** | Total request time | > 3000ms (p95) |
| **ai.cache.check** | Cache lookup time | > 50ms |
| **ai.context.search** | Vector search time | > 500ms |
| **ai.embedding.generate** | Embedding creation | > 200ms |
| **ai.prompt.build** | Prompt construction | > 100ms |
| **ai.llm.call** | LLM API call | > 2000ms (p95) |
| **ai.response.filter** | Output filtering | > 100ms |

## Advanced Observability Patterns

### Health Checks

```java
@Component
public class AIHealthIndicator implements HealthIndicator {
    
    private final ChatClient chatClient;
    private final VectorStore vectorStore;
    private final AIMetricsService metrics;
    
    @Override
    public Health health() {
        try {
            Health.Builder builder = new Health.Builder();
            
            // Check LLM connectivity
            boolean llmHealthy = checkLLMHealth();
            builder.withDetail("llm", llmHealthy ? "UP" : "DOWN");
            
            // Check vector store
            boolean vectorStoreHealthy = checkVectorStoreHealth();
            builder.withDetail("vectorStore", vectorStoreHealthy ? "UP" : "DOWN");
            
            // Check cache
            boolean cacheHealthy = checkCacheHealth();
            builder.withDetail("cache", cacheHealthy ? "UP" : "DOWN");
            
            // Check cost rate
            double costRate = metrics.getCurrentCostRate();
            builder.withDetail("costRate", String.format("$%.2f/hour", costRate));
            
            if (costRate > 100.0) {
                builder.withDetail("costWarning", "Cost rate exceeds threshold");
            }
            
            // Check active requests
            int activeRequests = metrics.getActiveRequestCount();
            builder.withDetail("activeRequests", activeRequests);
            
            if (activeRequests > 100) {
                builder.withDetail("loadWarning", "High number of active requests");
            }
            
            // Overall status
            if (llmHealthy && vectorStoreHealthy && cacheHealthy) {
                builder.up();
            } else {
                builder.down();
            }
            
            return builder.build();
            
        } catch (Exception e) {
            return Health.down()
                .withException(e)
                .build();
        }
    }
    
    private boolean checkLLMHealth() {
        try {
            String response = chatClient.prompt()
                .user("Health check")
                .options(OpenAiChatOptions.builder()
                    .withModel("gpt-3.5-turbo")
                    .withMaxTokens(10)
                    .build())
                .call()
                .content();
            
            return response != null && !response.isEmpty();
            
        } catch (Exception e) {
            log.error("LLM health check failed", e);
            return false;
        }
    }
    
    private boolean checkVectorStoreHealth() {
        try {
            vectorStore.similaritySearch(
                SearchRequest.query("test")
                    .withTopK(1)
            );
            return true;
            
        } catch (Exception e) {
            log.error("Vector store health check failed", e);
            return false;
        }
    }
    
    private boolean checkCacheHealth() {
        try {
            cache.get("health-check-key");
            return true;
            
        } catch (Exception e) {
            log.error("Cache health check failed", e);
            return false;
        }
    }
}
```

### Alerting Rules

```yaml
# Prometheus alerting rules
groups:
  - name: spring_ai_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighAIErrorRate
        expr: |
          rate(ai_requests_failure_total[5m]) / 
          rate(ai_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High AI error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over last 5 minutes"
      
      # High latency
      - alert: HighAILatency
        expr: |
          histogram_quantile(0.95, 
            rate(ai_request_duration_seconds_bucket[5m])) > 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "AI request latency is high"
          description: "95th percentile latency is {{ $value }}s"
      
      # Cost anomaly
      - alert: UnusualAICost
        expr: |
          rate(ai_cost_total[1h]) > 100
        for: 30m
        labels:
          severity: critical
        annotations:
          summary: "Unusual AI cost detected"
          description: "Cost rate is ${{ $value }}/hour"
      
      # Low cache hit rate
      - alert: LowCacheHitRate
        expr: |
          rate(ai_cache_hits[10m]) / 
          (rate(ai_cache_hits[10m]) + rate(ai_cache_misses[10m])) < 0.5
        for: 30m
        labels:
          severity: info
        annotations:
          summary: "Cache hit rate is low"
          description: "Cache hit rate is {{ $value | humanizePercentage }}"
      
      # Token usage spike
      - alert: TokenUsageSpike
        expr: |
          rate(ai_tokens_used_total[5m]) > 
          avg_over_time(rate(ai_tokens_used_total[5m])[1h:5m]) * 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Token usage spike detected"
          description: "Token usage is 3x normal rate"
```

### Real-Time Dashboard (Grafana)

```json
{
  "dashboard": {
    "title": "Spring AI Observability",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [{
          "expr": "rate(ai_requests_total[1m])"
        }],
        "type": "graph"
      },
      {
        "title": "Success Rate",
        "targets": [{
          "expr": "rate(ai_requests_success[5m]) / rate(ai_requests_total[5m]) * 100"
        }],
        "type": "stat",
        "thresholds": {
          "warning": 95,
          "critical": 90
        }
      },
      {
        "title": "Latency (p50, p95, p99)",
        "targets": [
          {"expr": "histogram_quantile(0.50, rate(ai_request_duration_seconds_bucket[5m]))"},
          {"expr": "histogram_quantile(0.95, rate(ai_request_duration_seconds_bucket[5m]))"},
          {"expr": "histogram_quantile(0.99, rate(ai_request_duration_seconds_bucket[5m]))"}
        ],
        "type": "graph"
      },
      {
        "title": "Cost Rate",
        "targets": [{
          "expr": "rate(ai_cost_total[1h]) * 3600"
        }],
        "type": "stat",
        "unit": "currencyUSD"
      },
      {
        "title": "Cache Performance",
        "targets": [
          {"expr": "rate(ai_cache_hits[5m])", "legendFormat": "Hits"},
          {"expr": "rate(ai_cache_misses[5m])", "legendFormat": "Misses"}
        ],
        "type": "graph"
      },
      {
        "title": "Token Usage by Model",
        "targets": [{
          "expr": "sum by (model) (rate(ai_tokens_used_total[5m]))"
        }],
        "type": "pie"
      },
      {
        "title": "Active Requests",
        "targets": [{
          "expr": "ai_requests_active"
        }],
        "type": "stat"
      },
      {
        "title": "Error Rate by Type",
        "targets": [{
          "expr": "sum by (type) (rate(ai_requests_failure[5m]))"
        }],
        "type": "bar"
      }
    ]
  }
}
```

## Observability Best Practices

### 1. Correlation IDs

```java
@Component
public class CorrelationIdFilter implements Filter {
    
    private static final String CORRELATION_ID_HEADER = "X-Correlation-ID";
    private static final String CORRELATION_ID_MDC_KEY = "correlation_id";
    
    @Override
    public void doFilter(ServletRequest request, ServletResponse response,
                        FilterChain chain) throws IOException, ServletException {
        
        HttpServletRequest httpRequest = (HttpServletRequest) request;
        
        // Get or generate correlation ID
        String correlationId = httpRequest.getHeader(CORRELATION_ID_HEADER);
        if (correlationId == null || correlationId.isEmpty()) {
            correlationId = UUID.randomUUID().toString();
        }
        
        // Add to MDC for logging
        MDC.put(CORRELATION_ID_MDC_KEY, correlationId);
        
        // Add to response headers
        HttpServletResponse httpResponse = (HttpServletResponse) response;
        httpResponse.setHeader(CORRELATION_ID_HEADER, correlationId);
        
        try {
            chain.doFilter(request, response);
        } finally {
            MDC.remove(CORRELATION_ID_MDC_KEY);
        }
    }
}
```

### 2. Sampling Strategy

```java
@Configuration
public class TracingSamplingConfiguration {
    
    /**
     * Custom sampling based on request characteristics
     */
    @Bean
    public Sampler customSampler() {
        return new Sampler() {
            private final Random random = new Random();
            
            @Override
            public SamplingResult shouldSample(
                    Context parentContext,
                    String traceId,
                    String name,
                    SpanKind spanKind,
                    Attributes attributes,
                    List<LinkData> parentLinks) {
                
                // Always sample errors
                if (attributes.get(AttributeKey.stringKey("error")) != null) {
                    return SamplingResult.recordAndSample();
                }
                
                // Always sample expensive requests
                Long cost = attributes.get(AttributeKey.longKey("cost_cents"));
                if (cost != null && cost > 10) {  // > $0.10
                    return SamplingResult.recordAndSample();
                }
                
                // Sample 10% of normal requests
                if (random.nextDouble() < 0.1) {
                    return SamplingResult.recordAndSample();
                }
                
                return SamplingResult.drop();
            }
            
            @Override
            public String getDescription() {
                return "CustomAISampler";
            }
        };
    }
}
```

### 3. Log Levels by Environment

```yaml
# application-dev.yml
logging:
  level:
    root: DEBUG
    org.springframework.ai: TRACE
    com.yourcompany.ai: TRACE

# application-prod.yml
logging:
  level:
    root: WARN
    org.springframework.ai: INFO
    com.yourcompany.ai: INFO
```

### 4. Performance Impact Monitoring

| Observability Component | Overhead | Mitigation |
|------------------------|----------|------------|
| **Metrics Collection** | < 1% CPU | Use efficient registries (Prometheus) |
| **Structured Logging** | 2-5% CPU | Async appenders, sampling |
| **Distributed Tracing** | 1-3% CPU | Sampling (10% in prod) |
| **MDC Context** | < 0.5% CPU | Clear after use |
| **Health Checks** | Negligible | Cache results, limit frequency |

## Debugging Common Issues

### Scenario 1: Intermittent Timeouts

**Symptoms:**
- Random 504 Gateway Timeout errors
- No clear pattern in logs

**Investigation using observability:**

```java
// Query traces for slow requests
SELECT * FROM traces 
WHERE duration > 30000 
  AND service = 'spring-ai-chatbot'
  AND span_name = 'ai.llm.call'
ORDER BY duration DESC
LIMIT 100;

// Check metrics
ai_request_duration_seconds{quantile="0.99"} > 30

// Analyze logs
{app="spring-ai-chatbot"} |= "AI interaction" | json | duration_ms > 30000
```

**Root cause identified:**
- OpenAI API occasionally takes >30s
- No retry logic implemented
- Timeout set too low (10s)

**Solution:**
```java
@Bean
public ChatClient chatClient() {
    return ChatClient.builder()
        .options(OpenAiChatOptions.builder()
            .withTimeout(Duration.ofSeconds(60))
            .withMaxRetries(3)
            .build())
        .build();
}
```

### Scenario 2: Unexpected Cost Spike

**Investigation:**

```promql
# Find users with highest costs
topk(10, sum by (user_id) (ai_cost_total))

# Identify expensive models
sum by (model) (ai_cost_total)

# Detect unusual patterns
rate(ai_cost_total[1h]) > 
  avg_over_time(rate(ai_cost_total[1h])[7d:1h]) * 2
```

**Root cause:**
- Single user making thousands of requests
- Using GPT-4 for simple queries that should use GPT-3.5

**Solution:**
- Implement model routing
- Add rate limiting per user
- Set cost budgets

### Scenario 3: Quality Degradation

**Investigation:**

```java
// Analyze response quality metrics
log.query("""
    {app="spring-ai-chatbot"} 
    |= "Response quality issue" 
    | json 
    | line_format "{{.model}}: {{.issue_type}}"
""");

// Check model distribution
SELECT model, COUNT(*) 
FROM ai_interactions 
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY model;
```

**Root cause:**
- Automatic failover to cheaper model
- Cheaper model quality insufficient for use case

**Solution:**
- Adjust model selection criteria
- Implement quality scoring
- Alert on quality drops

## Complete Observability Checklist

### Pre-Production

- ✅ All AI operations instrumented with metrics
- ✅ Structured logging configured
- ✅ Distributed tracing enabled
- ✅ Health checks implemented
- ✅ Cost tracking operational
- ✅ Security event logging active
- ✅ PII sanitization in logs
- ✅ Correlation IDs propagated
- ✅ Dashboard created
- ✅ Alerts configured

### Production

- ✅ Monitor dashboard daily
- ✅ Review cost reports weekly
- ✅ Analyze trace samples regularly
- ✅ Investigate anomalies promptly
- ✅ Tune alert thresholds
- ✅ Archive old logs (90+ days)
- ✅ Update dashboards as needed
- ✅ Document incidents
- ✅ Share learnings with team
- ✅ Iterate on observability

## Conclusion

**Without observability, you're flying blind.** You won't know when things break, why they break, or how much they're costing you.

**With comprehensive observability:**
- Detect issues in seconds, not hours
- Debug problems in minutes, not days
- Optimize costs continuously
- Ensure quality consistently
- Build confidence in your AI systems

### Key Takeaways

1. **Instrument everything**: Every AI operation should produce metrics, logs, and traces
2. **Use structured logging**: Make logs searchable and analyzable
3. **Trace end-to-end**: Connect user requests to API calls to responses
4. **Monitor costs**: Track spending in real-time, not after the bill arrives
5. **Alert proactively**: Detect problems before users notice
6. **Dashboard visibility**: Make metrics accessible to everyone
7. **Iterate continuously**: Improve observability as you learn

### Next Steps

**This Week:**
1. Add basic metrics to your AI operations
2. Configure structured logging
3. Set up a simple dashboard

**This Month:**
1. Implement distributed tracing
2. Create comprehensive dashboards
3. Configure alerts
4. Establish monitoring processes

**This Quarter:**
1. Fine-tune alert thresholds
2. Build custom metrics for your use case
3. Implement automated incident response
4. Share observability practices across teams

Remember: **Observability isn't a one-time setup—it's a continuous practice.** As your AI application evolves, your observability must evolve with it.

---

**Tools & Resources:**

- [Micrometer Documentation](https://micrometer.io/docs)
- [OpenTelemetry](https://opentelemetry.io/)
- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)
- [Jaeger Tracing](https://www.jaegertracing.io/)
- [Loki Log Aggregation](https://grafana.com/oss/loki/)

**Example Repository:**
- [Spring AI Observability Examples](https://github.com/spring-projects/spring-ai-examples)